{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f549d98-5268-4902-9880-79b47b6ebf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c7a68f-dcf9-46ca-98ee-3a4c24b402bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098e6a4b-4f69-4657-b1de-0fc09fd9a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pdb\n",
    "import dask\n",
    "from datetime import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask import delayed,compute\n",
    "import sys \n",
    "current_dir = os.getcwd()\n",
    "loading_dir = os.path.join(current_dir,\"data\",\"raw\",\n",
    "                           \"flash_crash_DJIA\",\"csv_files\")\n",
    "saving_dir = os.path.join(current_dir,\"data\",\"clean\",\"flash_crash\")\n",
    "src_path = current_dir + \"/src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bb89cc-4d27-4338-af5f-6aa86caeef44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loading_parallel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(loading_parallel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loading_parallel' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(loading_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b529343-cf8a-4bcb-ac47-98ca588132ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loading_parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aa528-817e-40e5-ae58-c9454893e269",
   "metadata": {},
   "source": [
    "#### Testing at first the extraction function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7134a1c-71e6-405d-b1eb-24bd9d2d0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AAPL.OQ-2010\"\n",
    "bbo_files, trade_files = loading_parallel.extract_files(loading_dir,\n",
    "                                       ticker,\n",
    "                                       \"2010-01-01\",\n",
    "                                       \"2010-01-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2804ba0-47e7-4f96-98ba-c129049e36d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/trade/2010_01/2010-01-01-AAPL.OQ-trade.csv.gz', '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/trade/2010_01/2010-01-04-AAPL.OQ-trade.csv.gz', '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/trade/2010_01/2010-01-05-AAPL.OQ-trade.csv.gz']\n",
      "['/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-05-AAPL.OQ-bbo.csv.gz', '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-04-AAPL.OQ-bbo.csv.gz', '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-01-AAPL.OQ-bbo.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "print(trade_files)\n",
    "print(bbo_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25bf5a-8d35-4582-9f0f-9a20bf10c6e3",
   "metadata": {},
   "source": [
    "## The function works well : now time to test loadTRTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf9669-7b6c-44ba-959b-3c5ba7bae9a3",
   "metadata": {},
   "source": [
    "#### This is an example on how to read one TRTH file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710dfbee-fc95-44b3-980d-2a881a5bdccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                     trade_price  trade_volume\n",
       " xltime                                                        \n",
       " 2010-01-04 09:30:00.576999424-05:00     213.5250           400\n",
       " 2010-01-04 09:30:00.616000-05:00        213.5375          1130\n",
       " 2010-01-04 09:30:00.635999488-05:00     213.5000           100\n",
       " 2010-01-04 09:30:00.648999936-05:00     213.5500           100\n",
       " 2010-01-04 09:30:00.676999680-05:00     213.5000           100\n",
       " ...                                          ...           ...\n",
       " 2010-01-04 15:59:59.526999808-05:00     214.2900          1231\n",
       " 2010-01-04 15:59:59.537999872-05:00     214.2800          2169\n",
       " 2010-01-04 15:59:59.746000128-05:00     214.3300           700\n",
       " 2010-01-04 15:59:59.828999680-05:00     214.2900           800\n",
       " 2010-01-04 15:59:59.988999936-05:00     214.2700          1901\n",
       " \n",
       " [18379 rows x 2 columns],)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trth_file = loading_parallel.load_TRTH_trade(trade_files[1])\n",
    "compute(trth_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385acfb-f017-4e41-9a03-a4bd6aef7d24",
   "metadata": {},
   "source": [
    "#### Example of reading too many files and concatenate them together : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef30ecd-acd0-4d3a-8018-bc37d661a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_TRTH_files(trade_files):\n",
    "    delayed_dfs = []\n",
    "    for file in trade_files:\n",
    "        delayed_df = delayed(loading_parallel.load_TRTH_trade)(file,\n",
    "                                              only_regular_trading_hours=True,\n",
    "                                              merge_sub_trades=True,\n",
    "                                              drop_columns=True)\n",
    "        delayed_dfs.append(delayed_df)\n",
    "    dfs = compute(*delayed_dfs)\n",
    "    final_df = pd.concat(dfs, ignore_index=False)\n",
    "    final_df = final_df.sort_index()\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49bd2071-a7eb-42c1-9398-9769d9867cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.66 ms, sys: 13 ms, total: 22.7 ms\n",
      "Wall time: 459 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_price</th>\n",
       "      <th>trade_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xltime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.576999424-05:00</th>\n",
       "      <td>213.5250</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.616000-05:00</th>\n",
       "      <td>213.5375</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.635999488-05:00</th>\n",
       "      <td>213.5000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.648999936-05:00</th>\n",
       "      <td>213.5500</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.676999680-05:00</th>\n",
       "      <td>213.5000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05 15:59:59.070999296-05:00</th>\n",
       "      <td>214.3200</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05 15:59:59.480999424-05:00</th>\n",
       "      <td>214.3700</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05 15:59:59.481999616-05:00</th>\n",
       "      <td>214.3700</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05 15:59:59.792999936-05:00</th>\n",
       "      <td>214.3500</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05 15:59:59.967000064-05:00</th>\n",
       "      <td>214.3700</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38353 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     trade_price  trade_volume\n",
       "xltime                                                        \n",
       "2010-01-04 09:30:00.576999424-05:00     213.5250           400\n",
       "2010-01-04 09:30:00.616000-05:00        213.5375          1130\n",
       "2010-01-04 09:30:00.635999488-05:00     213.5000           100\n",
       "2010-01-04 09:30:00.648999936-05:00     213.5500           100\n",
       "2010-01-04 09:30:00.676999680-05:00     213.5000           100\n",
       "...                                          ...           ...\n",
       "2010-01-05 15:59:59.070999296-05:00     214.3200           396\n",
       "2010-01-05 15:59:59.480999424-05:00     214.3700           685\n",
       "2010-01-05 15:59:59.481999616-05:00     214.3700           815\n",
       "2010-01-05 15:59:59.792999936-05:00     214.3500           200\n",
       "2010-01-05 15:59:59.967000064-05:00     214.3700          1118\n",
       "\n",
       "[38353 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "read_TRTH_files(trade_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc84ec5-d6c6-4814-9822-2e6351c3397b",
   "metadata": {},
   "source": [
    "#### Remark: The function works well with multiple entries : \n",
    "Let's measure the time to read January month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3f0184-f343-47e3-a3f0-b00ef832aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbo_files, trade_files = loading_parallel.extract_files(loading_dir,\n",
    "                                       ticker,\n",
    "                                       \"2010-01-01\",\n",
    "                                       \"2010-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3798e1cd-b6d4-4a68-96ea-07c4e2c82d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.2 ms, sys: 29.8 ms, total: 74 ms\n",
      "Wall time: 1.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_price</th>\n",
       "      <th>trade_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xltime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.576999424-05:00</th>\n",
       "      <td>213.525000</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.616000-05:00</th>\n",
       "      <td>213.537500</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.635999488-05:00</th>\n",
       "      <td>213.500000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.648999936-05:00</th>\n",
       "      <td>213.550000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.676999680-05:00</th>\n",
       "      <td>213.500000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.755999232-05:00</th>\n",
       "      <td>192.155000</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.772000-05:00</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.813999616-05:00</th>\n",
       "      <td>192.185714</td>\n",
       "      <td>4048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.845999616-05:00</th>\n",
       "      <td>192.090000</td>\n",
       "      <td>4100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.987999744-05:00</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     trade_price  trade_volume\n",
       "xltime                                                        \n",
       "2010-01-04 09:30:00.576999424-05:00   213.525000         400.0\n",
       "2010-01-04 09:30:00.616000-05:00      213.537500        1130.0\n",
       "2010-01-04 09:30:00.635999488-05:00   213.500000         100.0\n",
       "2010-01-04 09:30:00.648999936-05:00   213.550000         100.0\n",
       "2010-01-04 09:30:00.676999680-05:00   213.500000         100.0\n",
       "...                                          ...           ...\n",
       "2010-01-29 15:59:59.755999232-05:00   192.155000         300.0\n",
       "2010-01-29 15:59:59.772000-05:00      192.000000         200.0\n",
       "2010-01-29 15:59:59.813999616-05:00   192.185714        4048.0\n",
       "2010-01-29 15:59:59.845999616-05:00   192.090000        4100.0\n",
       "2010-01-29 15:59:59.987999744-05:00   192.000000        1300.0\n",
       "\n",
       "[562722 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "read_TRTH_files(trade_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62658f-6e5e-4f41-9683-629a49797b5b",
   "metadata": {},
   "source": [
    "#### Remark: Everithing works well, the program seems to be efficent : \n",
    "We read the data fast : (1 second to read more than 1  million data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ccea8-d5f2-467f-9dbc-8f1ed74926c5",
   "metadata": {},
   "source": [
    "## BBO testing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a296186a-2161-4b2c-a501-9ae105d88f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                     bid-price  bid-volume  ask-price  \\\n",
       " xltime                                                                  \n",
       " 2010-01-20 09:30:00.012998912-05:00     214.75           1     214.91   \n",
       " 2010-01-20 09:30:00.014999296-05:00     214.80           1     214.93   \n",
       " 2010-01-20 09:30:00.130999552-05:00     214.80           1     214.90   \n",
       " 2010-01-20 09:30:00.142999552-05:00     214.80           1     214.93   \n",
       " 2010-01-20 09:30:00.150999808-05:00     214.80           2     214.95   \n",
       " ...                                        ...         ...        ...   \n",
       " 2010-01-20 15:59:59.785999872-05:00     211.64          26     211.65   \n",
       " 2010-01-20 15:59:59.790999552-05:00     211.64          26     211.65   \n",
       " 2010-01-20 15:59:59.889999360-05:00     211.64          26     211.65   \n",
       " 2010-01-20 15:59:59.952999424-05:00     211.64          26     211.65   \n",
       " 2010-01-20 15:59:59.996999424-05:00     211.64          26     211.65   \n",
       " \n",
       "                                      ask-volume  \n",
       " xltime                                           \n",
       " 2010-01-20 09:30:00.012998912-05:00           1  \n",
       " 2010-01-20 09:30:00.014999296-05:00           1  \n",
       " 2010-01-20 09:30:00.130999552-05:00           1  \n",
       " 2010-01-20 09:30:00.142999552-05:00           1  \n",
       " 2010-01-20 09:30:00.150999808-05:00           1  \n",
       " ...                                         ...  \n",
       " 2010-01-20 15:59:59.785999872-05:00          21  \n",
       " 2010-01-20 15:59:59.790999552-05:00          11  \n",
       " 2010-01-20 15:59:59.889999360-05:00           8  \n",
       " 2010-01-20 15:59:59.952999424-05:00           7  \n",
       " 2010-01-20 15:59:59.996999424-05:00           1  \n",
       " \n",
       " [133555 rows x 4 columns],)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbo_example = loading_parallel.load_TRTH_bbo(bbo_files[0])\n",
    "compute(bbo_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76fc48-374e-4abf-a776-76c3669ce712",
   "metadata": {},
   "source": [
    "#### Now time to test on multiple bbo files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ed3710-6050-4968-b9c9-05bc642bee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bbo_files(trade_files):\n",
    "    delayed_dfs = []\n",
    "    for file in trade_files:\n",
    "        delayed_df = delayed(loading_parallel.load_TRTH_bbo)(file)\n",
    "        delayed_dfs.append(delayed_df)\n",
    "    dfs = compute(*delayed_dfs)\n",
    "    final_df = pd.concat(dfs, ignore_index=False)\n",
    "    final_df = final_df.sort_index()\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4156a4e-f733-4d81-88b9-2b4270b9ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 114 ms, total: 236 ms\n",
      "Wall time: 1.53 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid-price</th>\n",
       "      <th>bid-volume</th>\n",
       "      <th>ask-price</th>\n",
       "      <th>ask-volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xltime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.000999936-05:00</th>\n",
       "      <td>213.32</td>\n",
       "      <td>1</td>\n",
       "      <td>213.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.009000192-05:00</th>\n",
       "      <td>213.37</td>\n",
       "      <td>1</td>\n",
       "      <td>213.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.020000-05:00</th>\n",
       "      <td>213.38</td>\n",
       "      <td>2</td>\n",
       "      <td>213.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.042000128-05:00</th>\n",
       "      <td>213.39</td>\n",
       "      <td>1</td>\n",
       "      <td>213.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:30:00.087999744-05:00</th>\n",
       "      <td>213.40</td>\n",
       "      <td>1</td>\n",
       "      <td>213.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.845999616-05:00</th>\n",
       "      <td>191.97</td>\n",
       "      <td>2</td>\n",
       "      <td>191.98</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.889999360-05:00</th>\n",
       "      <td>191.97</td>\n",
       "      <td>2</td>\n",
       "      <td>191.98</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.932999936-05:00</th>\n",
       "      <td>191.97</td>\n",
       "      <td>1</td>\n",
       "      <td>191.98</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.997999616-05:00</th>\n",
       "      <td>191.97</td>\n",
       "      <td>1</td>\n",
       "      <td>192.12</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-29 15:59:59.998999808-05:00</th>\n",
       "      <td>191.97</td>\n",
       "      <td>1</td>\n",
       "      <td>192.12</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2546491 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     bid-price  bid-volume  ask-price  \\\n",
       "xltime                                                                  \n",
       "2010-01-04 09:30:00.000999936-05:00     213.32           1     213.42   \n",
       "2010-01-04 09:30:00.009000192-05:00     213.37           1     213.50   \n",
       "2010-01-04 09:30:00.020000-05:00        213.38           2     213.50   \n",
       "2010-01-04 09:30:00.042000128-05:00     213.39           1     213.50   \n",
       "2010-01-04 09:30:00.087999744-05:00     213.40           1     213.50   \n",
       "...                                        ...         ...        ...   \n",
       "2010-01-29 15:59:59.845999616-05:00     191.97           2     191.98   \n",
       "2010-01-29 15:59:59.889999360-05:00     191.97           2     191.98   \n",
       "2010-01-29 15:59:59.932999936-05:00     191.97           1     191.98   \n",
       "2010-01-29 15:59:59.997999616-05:00     191.97           1     192.12   \n",
       "2010-01-29 15:59:59.998999808-05:00     191.97           1     192.12   \n",
       "\n",
       "                                     ask-volume  \n",
       "xltime                                           \n",
       "2010-01-04 09:30:00.000999936-05:00           1  \n",
       "2010-01-04 09:30:00.009000192-05:00           3  \n",
       "2010-01-04 09:30:00.020000-05:00              1  \n",
       "2010-01-04 09:30:00.042000128-05:00           1  \n",
       "2010-01-04 09:30:00.087999744-05:00           1  \n",
       "...                                         ...  \n",
       "2010-01-29 15:59:59.845999616-05:00          51  \n",
       "2010-01-29 15:59:59.889999360-05:00          50  \n",
       "2010-01-29 15:59:59.932999936-05:00          50  \n",
       "2010-01-29 15:59:59.997999616-05:00          61  \n",
       "2010-01-29 15:59:59.998999808-05:00          60  \n",
       "\n",
       "[2546491 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "read_bbo_files(bbo_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c897bb4-d8f6-4ba4-86dd-659ea284c84a",
   "metadata": {},
   "source": [
    "#### Everything seems good : We are well using the load_bbo files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea8a74-b778-4dcc-ac7b-9155794fe7ea",
   "metadata": {},
   "source": [
    "## Now the big function to save: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd28b513-107f-480a-a3bd-d27f1543106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_merge_trade_bbo(ticker,date_start,\n",
    "                         date_end,\n",
    "                         dirBase,\n",
    "                         suffix=\"parquet\",\n",
    "                         suffix_save=None,\n",
    "                         dirSaveBase = None ,\n",
    "                         saveOnly=False,\n",
    "                         doSave=False):\n",
    "    \"\"\"\n",
    "    This function allows to merge trade and bbo together of a given ticker, \n",
    "    We should precise the destination to save the merged data \n",
    "    The date merged can be done in any period we would like to test on\n",
    "    By default the method returns the merged series, but can save or only save the merged series \n",
    "    Gien a location of the saving directory, the suffix is by default a parquet file. \n",
    "    \"\"\"\n",
    "    bbo_files,trade_files  = loading_parallel.extract_files(dirBase,ticker,date_start,date_end)\n",
    "    trades = read_TRTH_files(trade_files)\n",
    "    bbos = read_bbo_files(bbo_files)\n",
    "    try:\n",
    "        trades.shape + bbos.shape\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    events=trades.join(bbos,how=\"outer\")\n",
    "    \n",
    "    if doSave:\n",
    "        dirSave=os.path.join(dirSaveBase,ticker)\n",
    "        if not os.path.isdir(dirSave):\n",
    "            os.makedirs(dirSave)\n",
    "\n",
    "        if suffix_save:\n",
    "            suffix=suffix_save\n",
    "        \n",
    "        file_events=os.path.join(dirSave , date_start + date_end)\n",
    "       # pdb.set_trace()\n",
    "\n",
    "        saved=False\n",
    "        if suffix == \"arrow\":\n",
    "            # Convert Pandas DataFrame to Dask DataFrame\n",
    "            dask_df = dd.from_pandas(events, npartitions=1)  # Adjust npartitions based on your dataset size and memory\n",
    "            # Dask doesn't directly support exporting to Arrow, so you would need to use PyArrow separately if needed\n",
    "            # For now, you can save it as Parquet which is compatible with Arrow\n",
    "            dask_df.to_parquet(file_events+\".\"+suffix, write_index=True)  # Parquet format is compatible with Arrow\n",
    "            saved = True\n",
    "        \n",
    "        elif suffix == \"parquet\":\n",
    "            dask_df = dd.from_pandas(events, npartitions=1)  # Convert Pandas DataFrame to Dask DataFrame\n",
    "            dask_df.to_parquet(file_events, write_index=True, engine='pyarrow', write_metadata_file=True)\n",
    "            saved = True\n",
    "        \n",
    "        if not saved:\n",
    "            print(f\"suffix {suffix}: format not recognized\")\n",
    "\n",
    "        if saveOnly:\n",
    "            return saved\n",
    "    return events\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e7145d-4e39-442f-8ee3-548875da392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = load_merge_trade_bbo(\n",
    "    ticker=\"AAPL.OQ-2010\",\n",
    "    date_start=\"2010-01-01\",\n",
    "    date_end=\"2010-01-31\",\n",
    "    dirBase=loading_dir,\n",
    "    suffix=\"parquet\",\n",
    "    suffix_save=None,\n",
    "    dirSaveBase=saving_dir,\n",
    "    saveOnly=False,\n",
    "    doSave=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027ed6bd-94e1-4124-9df8-8b10188a777c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==12.0.1\n",
      "  Downloading pyarrow-12.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pyarrow==12.0.1) (1.26.3)\n",
      "Downloading pyarrow-12.0.1-cp311-cp311-macosx_11_0_arm64.whl (22.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.6/22.6 MB\u001b[0m \u001b[31m849.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "Successfully installed pyarrow-12.0.1\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: cchardet\n",
      "  Building wheel for cchardet (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/cchardet\n",
      "  \u001b[31m   \u001b[0m copying src/cchardet/version.py -> build/lib.macosx-11.1-arm64-cpython-311/cchardet\n",
      "  \u001b[31m   \u001b[0m copying src/cchardet/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/cchardet\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'cchardet._cchardet' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/cchardet\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/ext\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/ext/uchardet\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/ext/uchardet/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/ext/uchardet/src/LangModels\n",
      "  \u001b[31m   \u001b[0m clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ilyesbenayed/anaconda3/envs/bigData/include -arch arm64 -fPIC -O2 -isystem /Users/ilyesbenayed/anaconda3/envs/bigData/include -arch arm64 -Isrc/ext/uchardet/src -I/Users/ilyesbenayed/anaconda3/envs/bigData/include/python3.11 -c src/cchardet/_cchardet.cpp -o build/temp.macosx-11.1-arm64-cpython-311/src/cchardet/_cchardet.o\n",
      "  \u001b[31m   \u001b[0m src/cchardet/_cchardet.cpp:196:12: fatal error: 'longintrepr.h' file not found\n",
      "  \u001b[31m   \u001b[0m   #include \"longintrepr.h\"\n",
      "  \u001b[31m   \u001b[0m            ^~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m 1 error generated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for cchardet\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for cchardet\n",
      "Failed to build cchardet\n",
      "\u001b[31mERROR: Could not build wheels for cchardet, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==12.0.1\n",
    "!pip install cchardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7365a58-4f1b-4192-84d0-1c8412b402d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n",
      "/Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "file metadata is only available after writer close",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m, in \u001b[0;36mload_merge_trade_bbo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     dask_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(events, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert Pandas DataFrame to Dask DataFrame\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     dask_df\u001b[38;5;241m.\u001b[39mto_parquet(file_events, write_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m, write_metadata_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m     saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved:\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/io/parquet/core.py:1057\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1054\u001b[0m out \u001b[38;5;241m=\u001b[39m Scalar(graph, final_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[0;32m-> 1057\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompute_kwargs)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m fs\u001b[38;5;241m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/io/parquet/core.py:170\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m filename \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_function(part_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_offset)\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Write out data\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mwrite_partition(\n\u001b[1;32m    171\u001b[0m     df,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs,\n\u001b[1;32m    174\u001b[0m     filename,\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_on,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_metadata_file,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs_pass, head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m part_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs_pass),\n\u001b[1;32m    178\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:912\u001b[0m, in \u001b[0;36mwrite_partition\u001b[0;34m()\u001b[0m\n\u001b[1;32m    910\u001b[0m md_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(fs\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mjoin([path, filename]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fil:\n\u001b[0;32m--> 912\u001b[0m     pq\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[1;32m    913\u001b[0m         t,\n\u001b[1;32m    914\u001b[0m         fil,\n\u001b[1;32m    915\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    916\u001b[0m         metadata_collector\u001b[38;5;241m=\u001b[39mmd_list \u001b[38;5;28;01mif\u001b[39;00m return_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md_list:\n\u001b[1;32m    920\u001b[0m     _meta \u001b[38;5;241m=\u001b[39m md_list[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/core.py:3071\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3069\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3071\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3072\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[1;32m   3073\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   3074\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m   3075\u001b[0m             flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[1;32m   3076\u001b[0m             use_dictionary\u001b[38;5;241m=\u001b[39muse_dictionary,\n\u001b[1;32m   3077\u001b[0m             write_statistics\u001b[38;5;241m=\u001b[39mwrite_statistics,\n\u001b[1;32m   3078\u001b[0m             coerce_timestamps\u001b[38;5;241m=\u001b[39mcoerce_timestamps,\n\u001b[1;32m   3079\u001b[0m             data_page_size\u001b[38;5;241m=\u001b[39mdata_page_size,\n\u001b[1;32m   3080\u001b[0m             allow_truncated_timestamps\u001b[38;5;241m=\u001b[39mallow_truncated_timestamps,\n\u001b[1;32m   3081\u001b[0m             compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   3082\u001b[0m             use_deprecated_int96_timestamps\u001b[38;5;241m=\u001b[39muse_int96,\n\u001b[1;32m   3083\u001b[0m             compression_level\u001b[38;5;241m=\u001b[39mcompression_level,\n\u001b[1;32m   3084\u001b[0m             use_byte_stream_split\u001b[38;5;241m=\u001b[39muse_byte_stream_split,\n\u001b[1;32m   3085\u001b[0m             column_encoding\u001b[38;5;241m=\u001b[39mcolumn_encoding,\n\u001b[1;32m   3086\u001b[0m             data_page_version\u001b[38;5;241m=\u001b[39mdata_page_version,\n\u001b[1;32m   3087\u001b[0m             use_compliant_nested_type\u001b[38;5;241m=\u001b[39muse_compliant_nested_type,\n\u001b[1;32m   3088\u001b[0m             encryption_properties\u001b[38;5;241m=\u001b[39mencryption_properties,\n\u001b[1;32m   3089\u001b[0m             write_batch_size\u001b[38;5;241m=\u001b[39mwrite_batch_size,\n\u001b[1;32m   3090\u001b[0m             dictionary_pagesize_limit\u001b[38;5;241m=\u001b[39mdictionary_pagesize_limit,\n\u001b[1;32m   3091\u001b[0m             store_schema\u001b[38;5;241m=\u001b[39mstore_schema,\n\u001b[1;32m   3092\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m   3093\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/core.py:1018\u001b[0m, in \u001b[0;36m__exit__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;66;03m# return false since we want to propagate exceptions\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/core.py:1089\u001b[0m, in \u001b[0;36mclose\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_handle\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1790\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.metadata.__get__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m     result.init(metadata)\n\u001b[1;32m   1789\u001b[0m     return result\n\u001b[0;32m-> 1790\u001b[0m raise RuntimeError(\n\u001b[1;32m   1791\u001b[0m     'file metadata is only available after writer close')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: file metadata is only available after writer close"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compute(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169b43a-4b48-44ed-9215-96a82bfd697d",
   "metadata": {},
   "source": [
    "#### Now reading the file arrow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "42d14609-5ef4-4d41-b30a-12246d8f4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_dir = os.path.join(saving_dir,\"AAPL.OQ-2010\",\"2010-01-012010-01-31.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4be5e0bb-7819-40ac-8400-82399a7d93df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part.0.parquet']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checking_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987df4f-ae9b-42d5-82f6-1d6c33e51ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb187c07-86ed-41f9-9ce4-32a8d64474a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checking_dir,\n\u001b[1;32m      2\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart.0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m                      engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                      convert_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pandas/io/parquet.py:654\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    503\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    512\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    657\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pandas/io/parquet.py:77\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pandas/io/parquet.py:165\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     import_optional_dependency(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow is required for parquet support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401,E501\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/core.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParquetReader, Statistics,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                               FileMetaData, RowGroupMetaData,\n\u001b[1;32m     40\u001b[0m                               ColumnChunkMetaData,\n\u001b[1;32m     41\u001b[0m                               ParquetSchema, ColumnSchema,\n\u001b[1;32m     42\u001b[0m                               ParquetLogicalType,\n\u001b[1;32m     43\u001b[0m                               FileEncryptionProperties,\n\u001b[1;32m     44\u001b[0m                               FileDecryptionProperties)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LocalFileSystem, FileSystem, FileType,\n\u001b[1;32m     46\u001b[0m                         _resolve_filesystem_and_path, _ensure_filesystem)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filesystem \u001b[38;5;28;01mas\u001b[39;00m legacyfs\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1\u001b[0m, in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_parquet(os.path.join(checking_dir,\n",
    "                                  \"part.0.parquet\"),\n",
    "                     engine='pyarrow', \n",
    "                     convert_dates=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dfe4497-7587-4395-aec3-d7af3c792de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m      3\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetFile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checking_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart.0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(parquet_file\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/parquet/core.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParquetReader, Statistics,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                               FileMetaData, RowGroupMetaData,\n\u001b[1;32m     40\u001b[0m                               ColumnChunkMetaData,\n\u001b[1;32m     41\u001b[0m                               ParquetSchema, ColumnSchema,\n\u001b[1;32m     42\u001b[0m                               ParquetLogicalType,\n\u001b[1;32m     43\u001b[0m                               FileEncryptionProperties,\n\u001b[1;32m     44\u001b[0m                               FileDecryptionProperties)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LocalFileSystem, FileSystem, FileType,\n\u001b[1;32m     46\u001b[0m                         _resolve_filesystem_and_path, _ensure_filesystem)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filesystem \u001b[38;5;28;01mas\u001b[39;00m legacyfs\n",
      "File \u001b[0;32m~/anaconda3/envs/bigData/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1\u001b[0m, in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(os.path.join(checking_dir, \"part.0.parquet\"))\n",
    "print(parquet_file.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fe0b47e6-b3c9-4445-8729-7ba05e83b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: pyarrow in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (11.0.0)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-14.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: fastparquet in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (2023.10.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (2.7.0)\n",
      "Requirement already satisfied: fsspec in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (2023.12.2)\n",
      "Requirement already satisfied: packaging in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pyarrow-14.0.2-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "Successfully installed pyarrow-14.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas pyarrow fastparquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d14d0fd3-3bd4-4017-a793-581f03c0b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2023.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (1.26.3)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.7.0-cp311-cp311-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: fsspec in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (2023.12.2)\n",
      "Requirement already satisfied: packaging in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ilyesbenayed/anaconda3/envs/bigData/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2023.10.1-cp311-cp311-macosx_11_0_arm64.whl (682 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.7.0-cp311-cp311-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.7.0 fastparquet-2023.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd5ac3-7f3f-47b8-96a6-3245d5356b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
