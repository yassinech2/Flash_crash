{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "098e6a4b-4f69-4657-b1de-0fc09fd9a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pdb\n",
    "import dask\n",
    "from datetime import datetime\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7134a1c-71e6-405d-b1eb-24bd9d2d0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler=\"processes\")\n",
    "\n",
    "@dask.delayed\n",
    "def load_TRTH_trade(filename,\n",
    "             tz_exchange=\"America/New_York\",\n",
    "             only_non_special_trades=True,\n",
    "             only_regular_trading_hours=True,\n",
    "             open_time=\"09:30:00\",\n",
    "             close_time=\"16:00:00\",\n",
    "             merge_sub_trades=True,\n",
    "             drop_columns = True ):\n",
    "    try:\n",
    "        if re.search('(csv|csv\\\\.gz)$',filename):\n",
    "            DF = pd.read_csv(filename)\n",
    "        if re.search(r'arrow$',filename):\n",
    "            DF = pd.read_arrow(filename)\n",
    "        if re.search('parquet$',filename):\n",
    "            DF = pd.read_parquet(filename)\n",
    "\n",
    "    except Exception as e:\n",
    "     #   print(\"load_TRTH_trade could not load \"+filename)\n",
    "     #   print(e)\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        DF.shape\n",
    "    except Exception as e: # DF does not exist\n",
    "        print(\"DF does not exist\")\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    if DF.shape[0]==0:\n",
    "        return None\n",
    "    \n",
    "    if only_non_special_trades:\n",
    "        DF = DF[DF[\"trade-stringflag\"]==\"uncategorized\"]\n",
    "    if drop_columns==True: \n",
    "        DF.drop(columns=[\"trade-rawflag\",\"trade-stringflag\"],axis=1,inplace=True)\n",
    "    \n",
    "    DF.index = pd.to_datetime(DF[\"xltime\"],unit=\"d\",origin=\"1899-12-30\",utc=True)\n",
    "    DF.index = DF.index.tz_convert(tz_exchange)  # .P stands for Arca, which is based at New York\n",
    "    DF.drop(columns=\"xltime\",inplace=True)\n",
    "    \n",
    "    if only_regular_trading_hours:\n",
    "        DF=DF.between_time(open_time,close_time)    # warning: ever heard e.g. about Thanksgivings?\n",
    "    \n",
    "    if merge_sub_trades:\n",
    "           DF=DF.groupby(DF.index).agg(trade_price=pd.NamedAgg(column='trade-price', aggfunc='mean'),\n",
    "                                       trade_volume=pd.NamedAgg(column='trade-volume', aggfunc='sum'))\n",
    "    \n",
    "    return DF\n",
    "\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def load_TRTH_bbo(filename,\n",
    "             tz_exchange=\"America/New_York\",\n",
    "             only_regular_trading_hours=True,\n",
    "             merge_sub_trades=True):\n",
    "    try:\n",
    "        if re.search(r'(csv|csv\\.gz)$',filename):\n",
    "            DF = pd.read_csv(filename)\n",
    "        if re.search(r'arrow$',filename):\n",
    "            DF = pd.read_arrow(filename)\n",
    "        if re.search(r'parquet$',filename):\n",
    "            DF = pd.read_parquet(filename) \n",
    "    except Exception as e:\n",
    "       # print(\"load_TRTH_bbo could not load \"+filename)\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        DF.shape\n",
    "    except Exception as e: # DF does not exist\n",
    "        print(\"DF does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5384540-2651-4b92-beba-9048740e0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(base_dir, ticker, start_date, end_date):\n",
    "    # Convert start and end dates to datetime objects for comparison\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Paths to search\n",
    "    paths_to_search = [os.path.join(base_dir, ticker, 'bbo'), os.path.join(base_dir, ticker, 'trade')]\n",
    "\n",
    "    # List to hold the paths of the files that match the criteria\n",
    "    matching_files = []\n",
    "\n",
    "    for path in paths_to_search:\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            # Check if current directory matches the required format 'yyyy_mm'\n",
    "            current_dir = os.path.basename(root)\n",
    "            try:\n",
    "                dir_year, dir_month = current_dir.split('_')\n",
    "                dir_date = datetime(int(dir_year), int(dir_month), 1)\n",
    "                if start_date <= dir_date <= end_date:\n",
    "                    for file in files:\n",
    "                        # Parse the date from the file name and check if it falls within the date range\n",
    "                        try:\n",
    "                            file_date_str = '-'.join(file.split('-')[:3])\n",
    "                            file_date = datetime.strptime(file_date_str, \"%Y-%m-%d\")\n",
    "                            if start_date <= file_date <= end_date:\n",
    "                                full_path = os.path.join(root, file)\n",
    "                                matching_files.append(full_path)\n",
    "                        except ValueError:\n",
    "                            # Skip files that do not match the expected date format\n",
    "                            pass\n",
    "            except ValueError:\n",
    "                # Skip directories that do not match the expected format 'yyyy_mm'\n",
    "                pass\n",
    "\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2804ba0-47e7-4f96-98ba-c129049e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(base_dir, ticker, start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    bbo_files = []\n",
    "    trade_files = []\n",
    "\n",
    "    paths_to_search = {\n",
    "        'bbo': os.path.join(base_dir, ticker, 'bbo'),\n",
    "        'trade': os.path.join(base_dir, ticker, 'trade')\n",
    "    }\n",
    "\n",
    "    for category, path in paths_to_search.items():\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            current_dir = os.path.basename(root)\n",
    "            try:\n",
    "                dir_year, dir_month = current_dir.split('_')\n",
    "                dir_date = datetime(int(dir_year), int(dir_month), 1)\n",
    "                if start_date <= dir_date <= end_date:\n",
    "                    for file in files:\n",
    "                        file_date_str = '-'.join(file.split('-')[:3])\n",
    "                        try:\n",
    "                            file_date = datetime.strptime(file_date_str, \"%Y-%m-%d\")\n",
    "                            if start_date <= file_date <= end_date:\n",
    "                                full_path = os.path.join(root, file)\n",
    "                                if category == 'bbo':\n",
    "                                    bbo_files.append(full_path)\n",
    "                                else:\n",
    "                                    trade_files.append(full_path)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return bbo_files, trade_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "710dfbee-fc95-44b3-980d-2a881a5bdccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-05-AAPL.OQ-bbo.csv.gz',\n",
       " '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-06-AAPL.OQ-bbo.csv.gz',\n",
       " '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-04-AAPL.OQ-bbo.csv.gz',\n",
       " '/Users/ilyesbenayed/Desktop/Big data/data/raw/flash_crash_DJIA/csv_files/AAPL.OQ-2010/bbo/2010_01/2010-01-01-AAPL.OQ-bbo.csv.gz']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "loading_dir = os.path.join(current_dir,\"data\",\"raw\",\n",
    "                                    \"flash_crash_DJIA\",\"csv_files\")\n",
    "#print(os.listdir(loading_dir))\n",
    "example_ticker = \"AAPL.OQ-2010\"\n",
    "bbo_files, trade_files = extract_files(loading_dir,example_ticker,\"2010-01-01\",\"2010-01-06\")\n",
    "bbo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee7b86-3d68-4213-bf15-90b69e65c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_dfs = []\n",
    "\n",
    "for file in os.listdir(stock_dir_ex):\n",
    "    path_file = os.path.join(stock_dir_ex, file)\n",
    "    # Delay computation\n",
    "    delayed_df = delayed(load_TRTH_trade)(path_file,only_regular_trading_hours=False,merge_sub_trades=False,drop_columns=False)\n",
    "    delayed_dfs.append(delayed_df)\n",
    "\n",
    "# Compute all DataFrames in parallel and concatenate\n",
    "dfs = compute(*delayed_dfs)\n",
    "final_df = pd.concat(dfs, ignore_index=False)\n",
    "final_df = final_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28b513-107f-480a-a3bd-d27f1543106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_merge_trade_bbo(ticker,date_start,\n",
    "                         date_end,\n",
    "                         dirBase,\n",
    "                         suffix=\"parquet\",\n",
    "                         suffix_save=None,\n",
    "                         dirSaveBase = None ,\n",
    "                         saveOnly=False,\n",
    "                         doSave=False\n",
    "                        ):\n",
    "    \"\"\"\"\n",
    "    This function allows to merge trade and bbo together of a given ticker, \n",
    "    We should precise the destination to save the merged data \n",
    "    The date merged can be done in any period we would like to test on\n",
    "    By default the method returns the merged series, but can save or only save the merged series \n",
    "    Gien a location of the saving directory, the suffix is by default a parquet file. \n",
    "    \"\"\"\"\n",
    "    bbo_files,trade_files  = extract_files(dirBase,ticker,date_start,date_end)\n",
    "    trade_dfs = []\n",
    "\n",
    "    for file in trade_files:\n",
    "        # Delay computation\n",
    "        trade_df = delayed(load_TRTH_trade)(file,only_regular_trading_hours=False,merge_sub_trades=False,drop_columns=False)\n",
    "        trade_dfs.append(trade_df)\n",
    "    # Compute all DataFrames in parallel and concatenate\n",
    "    dfs = compute(*trade_dfs)\n",
    "    trades = pd.concat(dfs, ignore_index=False)\n",
    "    trades = trades.sort_index()\n",
    "\n",
    "    bbo_dfs = []\n",
    "    for file in bbo_files:\n",
    "        bbo_df =  delayed(load_TRTH_bbo)(file)\n",
    "        bbo_dfs.append(bbo_df)\n",
    "    dfs = compute(*bbo_dfs)\n",
    "    bbos  = pd.concat(dfs,ignore_index= False)\n",
    "    bbos = bbos.sort_index()\n",
    "    try:\n",
    "        trades.shape + bbos.shape\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    events=trades.join(bbos,how=\"outer\")\n",
    "    \n",
    "    if doSave:\n",
    "        dirSave=os.path.join(dirSaveBase,ticker)\n",
    "        if not os.path.isdir(dirSave):\n",
    "            os.makedirs(dirSave)\n",
    "\n",
    "        if suffix_save:\n",
    "            suffix=suffix_save\n",
    "        \n",
    "        file_events=os.path.join(dirSave,\"events.\"+suffix)\n",
    "       # pdb.set_trace()\n",
    "\n",
    "        saved=False\n",
    "        if suffix == \"arrow\":\n",
    "            # Convert Pandas DataFrame to Dask DataFrame\n",
    "            dask_df = dd.from_pandas(events, npartitions=1)  # Adjust npartitions based on your dataset size and memory\n",
    "            # Dask doesn't directly support exporting to Arrow, so you would need to use PyArrow separately if needed\n",
    "            # For now, you can save it as Parquet which is compatible with Arrow\n",
    "            dask_df.to_parquet(file_events, write_index=True)  # Parquet format is compatible with Arrow\n",
    "            saved = True\n",
    "        \n",
    "        elif suffix == \"parquet\":\n",
    "            dask_df = dd.from_pandas(events, npartitions=1)  # Convert Pandas DataFrame to Dask DataFrame\n",
    "            dask_df.to_parquet(file_events, write_index=True, engine='pyarrow', write_metadata_file=True)\n",
    "            saved = True\n",
    "        \n",
    "        if not saved:\n",
    "            print(f\"suffix {suffix}: format not recognized\")\n",
    "\n",
    "        if saveOnly:\n",
    "            return saved\n",
    "    return events\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "To be tested and completed \n",
    "\n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
